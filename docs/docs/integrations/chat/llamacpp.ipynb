{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cf960ef-8cf2-4d2c-8dc4-e7a34b76bd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U --quiet llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1dbe59-3d32-4bcd-bf4d-851f28447126",
   "metadata": {},
   "source": [
    "### Repo\n",
    "\n",
    "Clone and build Llama.cpp, following instructions here:\n",
    "\n",
    "https://github.com/ggerganov/llama.cpp\n",
    "\n",
    "### Model\n",
    "\n",
    "Download a local LLM, ideally one that is capable of tool calling to use all features discussed below:\n",
    " \n",
    "* Weights: `meta-llama-3-8b-instruct.Q8_0.gguf`\n",
    "* Link: https://huggingface.co/SanctumAI/Meta-Llama-3-8B-Instruct-GGUF\n",
    "\n",
    "### Function calling\n",
    "\n",
    "First, we can see that llama-cpp-python does support function calling.\n",
    "\n",
    "https://llama-cpp-python.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6462b386-689f-4a46-b0dc-c547a5c84105",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model = \"/Users/rlm/Desktop/Code/llama.cpp/models/meta-llama-3-8b-instruct.Q8_0.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1efae91-02a4-4527-9485-16957ae6970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatLlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9191f3a0-0525-4c03-ba25-12ff34c0fd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /Users/rlm/Desktop/Code/llama.cpp/models/meta-llama-3-8b-instruct.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  1416.45 MiB, ( 1416.52 / 21845.34)\n",
      "llm_load_tensors: offloading 4 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 4/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  8137.64 MiB\n",
      "llm_load_tensors:      Metal buffer size =  1416.44 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 10016\n",
      "llama_new_context_with_model: n_batch    = 200\n",
      "llama_new_context_with_model: n_ubatch   = 200\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Max\n",
      "ggml_metal_init: picking default device: Apple M2 Max\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1095.50 MiB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   156.50 MiB\n",
      "llama_new_context_with_model: KV self size  = 1252.00 MiB, K (f16):  626.00 MiB, V (f16):  626.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   265.59 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   265.59 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128009', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.pre': 'llama-bpe', 'tokenizer.ggml.model': 'gpt2', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'llama.context_length': '8192', 'llama.attention.head_count': '32', 'general.file_type': '7', 'llama.feed_forward_length': '14336', 'llama.rope.dimension_count': '128', 'llama.rope.freq_base': '500000.000000', 'llama.embedding_length': '4096', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'Meta-Llama-3-8B-Instruct', 'llama.block_count': '32'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm = ChatLlamaCpp(\n",
    "    temperature=0.3,\n",
    "    model_path=local_model,\n",
    "    n_ctx=10000,\n",
    "    n_gpu_layers=4,\n",
    "    n_batch=200,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    max_tokens=512,\n",
    "    n_threads=multiprocessing.cpu_count() - 1,\n",
    "    callback_manager=CallbackManager(\n",
    "        [StreamingStdOutCallbackHandler()]\n",
    "    ),  # Callbacks support token-wise streaming\n",
    "    streaming=True,\n",
    "    repeat_penalty=1.5,\n",
    "    top_p=0.5,\n",
    "    stop=[\"<|end_of_text|>\", \"<|eot_id|>\"],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58ab04f-08ae-4715-a9e1-50992bcea214",
   "metadata": {},
   "source": [
    "### Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49757b24-1aa9-4d55-8fd6-d4a58ae19b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je adore le programmation.\n",
      "\n",
      "(Note: \"programming\" is translated as both an activity (\"le programme\") and also referring specifically to computer code, which in this case I chose not translate.)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2102.08 ms\n",
      "llama_print_timings:      sample time =      10.86 ms /    39 runs   (    0.28 ms per token,  3591.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2101.88 ms /    36 tokens (   58.39 ms per token,    17.13 tokens per second)\n",
      "llama_print_timings:        eval time =    4366.03 ms /    38 runs   (  114.90 ms per token,     8.70 tokens per second)\n",
      "llama_print_timings:       total time =    6516.86 ms /    74 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Je adore le programmation.\\n\\n(Note: \"programming\" is translated as both an activity (\"le programme\") and also referring specifically to computer code, which in this case I chose not translate.)', response_metadata={'finish_reason': 'stop'}, id='run-c6d117d2-0478-4f45-9a8b-f84749225ecb-0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8698eb87-8f0d-4295-84b5-221f372090f4",
   "metadata": {},
   "source": [
    "### Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65945eef-5d04-4f23-8db6-fdf19a8ec8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ich liebe auch Programmieren! (Translation: I also like coding!) What kind of programs do you enjoy working on?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2102.08 ms\n",
      "llama_print_timings:      sample time =       6.91 ms /    25 runs   (    0.28 ms per token,  3616.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     605.25 ms /    16 tokens (   37.83 ms per token,    26.44 tokens per second)\n",
      "llama_print_timings:        eval time =    2692.54 ms /    24 runs   (  112.19 ms per token,     8.91 tokens per second)\n",
      "llama_print_timings:       total time =    3327.85 ms /    40 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ich liebe auch Programmieren! (Translation: I also like coding!) What kind of programs do you enjoy working on?', response_metadata={'finish_reason': 'stop'}, id='run-895aeed1-2460-4f46-9f99-6ce724bf5a8d-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"German\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0578a02a-a861-476f-a2db-ec496045ab9f",
   "metadata": {},
   "source": [
    "### Structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25927804-e291-4383-9691-cad9c880617d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "answer-kv ::= [\"] [a] [n] [s] [w] [e] [r] [\"] space [:] space string \n",
      "space ::= space_7 \n",
      "string ::= [\"] string_8 [\"] space \n",
      "char ::= [^\"\\] | [\\] char_4 \n",
      "char_4 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "justification-kv ::= [\"] [j] [u] [s] [t] [i] [f] [i] [c] [a] [t] [i] [o] [n] [\"] space [:] space string \n",
      "root ::= [{] space answer-kv [,] space justification-kv [}] space \n",
      "space_7 ::= [ ] | \n",
      "string_8 ::= char string_8 | \n",
      "\n",
      "llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2102.08 ms\n",
      "llama_print_timings:      sample time =     274.45 ms /    31 runs   (    8.85 ms per token,   112.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     673.28 ms /    20 tokens (   33.66 ms per token,    29.71 tokens per second)\n",
      "llama_print_timings:        eval time =    3262.23 ms /    30 runs   (  108.74 ms per token,     9.20 tokens per second)\n",
      "llama_print_timings:       total time =    4334.07 ms /    50 tokens\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    '''An answer to the user question along with justification for the answer.'''\n",
    "    answer: str\n",
    "    justification: str\n",
    "\n",
    "dict_schema = convert_to_openai_tool(AnswerWithJustification)\n",
    "structured_llm = llm.with_structured_output(dict_schema)\n",
    "result = structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2625fecc-d242-4110-a5c5-6b8b3c3accc9",
   "metadata": {},
   "source": [
    "### Tool calling\n",
    "\n",
    "* However, it cannot automatically trigger a function/tool. \n",
    "* We need to force it by specifying the 'tool choice' parameter. This parameter is typically formatted as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c85a76ea-1c7b-4df7-81ca-40ba150acf02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "char ::= [^\"\\] | [\\] char_1 \n",
      "char_1 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "location-kv ::= [\"] [l] [o] [c] [a] [t] [i] [o] [n] [\"] space [:] space string \n",
      "space ::= space_7 \n",
      "string ::= [\"] string_8 [\"] space \n",
      "root ::= [{] space location-kv [,] space unit-kv [}] space \n",
      "unit-kv ::= [\"] [u] [n] [i] [t] [\"] space [:] space unit \n",
      "space_7 ::= [ ] | \n",
      "string_8 ::= char string_8 | \n",
      "unit ::= [\"] [c] [e] [l] [s] [i] [u] [s] [\"] | [\"] [f] [a] [h] [r] [e] [n] [h] [e] [i] [t] [\"] \n",
      "\n",
      "llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2102.08 ms\n",
      "llama_print_timings:      sample time =     142.78 ms /    18 runs   (    7.93 ms per token,   126.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     501.87 ms /    14 tokens (   35.85 ms per token,    27.90 tokens per second)\n",
      "llama_print_timings:        eval time =    1747.80 ms /    17 runs   (  102.81 ms per token,     9.73 tokens per second)\n",
      "llama_print_timings:       total time =    2470.29 ms /    31 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weather', 'arguments': '{ \"location\": \"Napa Valley\", \"unit\" : \"fahrenheit\"}'}, 'tool_calls': [{'index': 0, 'id': 'call__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2c', 'type': 'functionfunctionfunctionfunctionfunctionfunctionfunctionfunctionfunctionfunctionfunctionfunctionfunctionfunctionfunctionfunctionfunctionfunction', 'function': {'name': 'get_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weather', 'arguments': '{ \"location\": \"Napa Valley\", \"unit\" : \"fahrenheit\"}'}}]}, response_metadata={'finish_reason': 'tool_calls'}, id='run-cb899151-5dbe-411c-995b-4a15e82039d2-0', tool_calls=[{'name': 'get_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weatherget_current_weather', 'args': {'location': 'Napa Valley', 'unit': 'fahrenheit'}, 'id': 'call__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2ccall__0_get_current_weather_cmpl-3e181b53-e014-4d5e-b9b0-c5313eebae2c'}])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class WeatherInput(BaseModel):\n",
    "    location: str = Field(description=\"The city and state, e.g. San Francisco, CA\")\n",
    "    unit: str = Field(enum=[\"celsius\", \"fahrenheit\"])\n",
    "\n",
    "@tool(\"get_current_weather\", args_schema=WeatherInput)\n",
    "def get_weather(location: str, unit: str):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    return f\"Now the weather in {location} is 22 {unit}\"\n",
    "\n",
    "llm_with_tools = llm.bind_tools(\n",
    "    tools=[get_weather],\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\"}},\n",
    ")\n",
    "\n",
    "ai_msg = llm_with_tools.invoke(\n",
    "    \"what is the weather in San Francisco, CA\",\n",
    ")\n",
    "\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db7cbb9-f97b-4355-aae5-647bbae14179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
