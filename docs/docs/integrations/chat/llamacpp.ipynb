{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cf960ef-8cf2-4d2c-8dc4-e7a34b76bd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U --quiet llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1dbe59-3d32-4bcd-bf4d-851f28447126",
   "metadata": {},
   "source": [
    "### Repo\n",
    "\n",
    "Clone and build Llama.cpp, following instructions here:\n",
    "\n",
    "https://github.com/ggerganov/llama.cpp\n",
    "\n",
    "### Model\n",
    "\n",
    "Download a local LLM, ideally one that is capable of tool calling to use all features discussed below:\n",
    " \n",
    "* Weights: `meta-llama-3-8b-instruct.Q8_0.gguf`\n",
    "* Link: https://huggingface.co/SanctumAI/Meta-Llama-3-8B-Instruct-GGUF\n",
    "\n",
    "### Function calling\n",
    "\n",
    "First, we can see that llama-cpp-python does support function calling.\n",
    "\n",
    "https://llama-cpp-python.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6462b386-689f-4a46-b0dc-c547a5c84105",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model = \"/Users/rlm/Desktop/Code/llama.cpp/models/meta-llama-3-8b-instruct.Q8_0.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b90bf-0cff-41b3-aab4-f0737905d621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=local_model, chat_format=\"chatml-function-calling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe6595b4-221e-46eb-bc0c-c1290d1010f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /Users/rlm/Desktop/Code/llama.cpp/models/meta-llama-3-8b-instruct.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  8137.64 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128009', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.pre': 'llama-bpe', 'tokenizer.ggml.model': 'gpt2', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'llama.context_length': '8192', 'llama.attention.head_count': '32', 'general.file_type': '7', 'llama.feed_forward_length': '14336', 'llama.rope.dimension_count': '128', 'llama.rope.freq_base': '500000.000000', 'llama.embedding_length': '4096', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'Meta-Llama-3-8B-Instruct', 'llama.block_count': '32'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "ggml_metal_free: deallocating\n",
      "from_string grammar:\n",
      "age-kv ::= [\"] [a] [g] [e] [\"] space [:] space integer \n",
      "space ::= space_41 \n",
      "integer ::= integer_5 space \n",
      "char ::= [^\"\\] | [\\] char_4 \n",
      "char_4 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "integer_5 ::= integer_6 integral-part \n",
      "integer_6 ::= [-] | \n",
      "integral-part ::= [0-9] | [1-9] integral-part_37 \n",
      "integral-part_8 ::= [0-9] integral-part_36 \n",
      "integral-part_9 ::= [0-9] integral-part_35 \n",
      "integral-part_10 ::= [0-9] integral-part_34 \n",
      "integral-part_11 ::= [0-9] integral-part_33 \n",
      "integral-part_12 ::= [0-9] integral-part_32 \n",
      "integral-part_13 ::= [0-9] integral-part_31 \n",
      "integral-part_14 ::= [0-9] integral-part_30 \n",
      "integral-part_15 ::= [0-9] integral-part_29 \n",
      "integral-part_16 ::= [0-9] integral-part_28 \n",
      "integral-part_17 ::= [0-9] integral-part_27 \n",
      "integral-part_18 ::= [0-9] integral-part_26 \n",
      "integral-part_19 ::= [0-9] integral-part_25 \n",
      "integral-part_20 ::= [0-9] integral-part_24 \n",
      "integral-part_21 ::= [0-9] integral-part_23 \n",
      "integral-part_22 ::= [0-9] \n",
      "integral-part_23 ::= integral-part_22 | \n",
      "integral-part_24 ::= integral-part_21 | \n",
      "integral-part_25 ::= integral-part_20 | \n",
      "integral-part_26 ::= integral-part_19 | \n",
      "integral-part_27 ::= integral-part_18 | \n",
      "integral-part_28 ::= integral-part_17 | \n",
      "integral-part_29 ::= integral-part_16 | \n",
      "integral-part_30 ::= integral-part_15 | \n",
      "integral-part_31 ::= integral-part_14 | \n",
      "integral-part_32 ::= integral-part_13 | \n",
      "integral-part_33 ::= integral-part_12 | \n",
      "integral-part_34 ::= integral-part_11 | \n",
      "integral-part_35 ::= integral-part_10 | \n",
      "integral-part_36 ::= integral-part_9 | \n",
      "integral-part_37 ::= integral-part_8 | \n",
      "name-kv ::= [\"] [n] [a] [m] [e] [\"] space [:] space string \n",
      "string ::= [\"] string_42 [\"] space \n",
      "root ::= [{] space name-kv [,] space age-kv [}] space \n",
      "space_41 ::= [ ] | \n",
      "string_42 ::= char string_42 | \n",
      "\n",
      "\n",
      "llama_print_timings:        load time =    7857.45 ms\n",
      "llama_print_timings:      sample time =     106.85 ms /    13 runs   (    8.22 ms per token,   121.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7857.15 ms /   268 tokens (   29.32 ms per token,    34.11 tokens per second)\n",
      "llama_print_timings:        eval time =    2054.21 ms /    12 runs   (  171.18 ms per token,     5.84 tokens per second)\n",
      "llama_print_timings:       total time =   10066.96 ms /   280 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-52b952a7-6847-4b6d-88b3-102c0dcb49e5',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1717799438,\n",
       " 'model': '/Users/rlm/Desktop/Code/llama.cpp/models/meta-llama-3-8b-instruct.Q8_0.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': None,\n",
       "    'function_call': {'name': 'UserDetail',\n",
       "     'arguments': '{\"name\": \"Jason\", \"age\": 25}'},\n",
       "    'tool_calls': [{'id': 'call__0_UserDetail_cmpl-52b952a7-6847-4b6d-88b3-102c0dcb49e5',\n",
       "      'type': 'function',\n",
       "      'function': {'name': 'UserDetail',\n",
       "       'arguments': '{\"name\": \"Jason\", \"age\": 25}'}}]},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'tool_calls'}],\n",
       " 'usage': {'prompt_tokens': 268, 'completion_tokens': 12, 'total_tokens': 280}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.create_chat_completion(\n",
    "      messages = [\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. The assistant calls functions with appropriate input when necessary\"\n",
    "\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"Extract Jason is 25 years old\"\n",
    "        }\n",
    "      ],\n",
    "      tools=[{\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "          \"name\": \"UserDetail\",\n",
    "          \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"title\": \"UserDetail\",\n",
    "            \"properties\": {\n",
    "              \"name\": {\n",
    "                \"title\": \"Name\",\n",
    "                \"type\": \"string\"\n",
    "              },\n",
    "              \"age\": {\n",
    "                \"title\": \"Age\",\n",
    "                \"type\": \"integer\"\n",
    "              }\n",
    "            },\n",
    "            \"required\": [ \"name\", \"age\" ]\n",
    "          }\n",
    "        }\n",
    "      }],\n",
    "      tool_choice={\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "          \"name\": \"UserDetail\"\n",
    "        }\n",
    "      }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e954e-82f4-4953-925f-b366065234f3",
   "metadata": {},
   "source": [
    "### Test with ChatLlamaCpp integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1efae91-02a4-4527-9485-16957ae6970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatLlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9191f3a0-0525-4c03-ba25-12ff34c0fd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /Users/rlm/Desktop/Code/llama.cpp/models/meta-llama-3-8b-instruct.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  1416.45 MiB, ( 1417.58 / 21845.34)\n",
      "llm_load_tensors: offloading 4 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 4/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  8137.64 MiB\n",
      "llm_load_tensors:      Metal buffer size =  1416.44 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 10016\n",
      "llama_new_context_with_model: n_batch    = 200\n",
      "llama_new_context_with_model: n_ubatch   = 200\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Max\n",
      "ggml_metal_init: picking default device: Apple M2 Max\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1095.50 MiB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   156.50 MiB\n",
      "llama_new_context_with_model: KV self size  = 1252.00 MiB, K (f16):  626.00 MiB, V (f16):  626.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   265.59 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   265.59 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128009', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.pre': 'llama-bpe', 'tokenizer.ggml.model': 'gpt2', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'llama.context_length': '8192', 'llama.attention.head_count': '32', 'general.file_type': '7', 'llama.feed_forward_length': '14336', 'llama.rope.dimension_count': '128', 'llama.rope.freq_base': '500000.000000', 'llama.embedding_length': '4096', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'Meta-Llama-3-8B-Instruct', 'llama.block_count': '32'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm = ChatLlamaCpp(\n",
    "    temperature=0.3,\n",
    "    model_path=local_model,\n",
    "    n_ctx=10000,\n",
    "    n_gpu_layers=4,\n",
    "    n_batch=200,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    max_tokens=512,\n",
    "    n_threads=multiprocessing.cpu_count() - 1,\n",
    "    callback_manager=CallbackManager(\n",
    "        [StreamingStdOutCallbackHandler()]\n",
    "    ),  # Callbacks support token-wise streaming\n",
    "    streaming=True,\n",
    "    repeat_penalty=1.5,\n",
    "    top_p=0.5,\n",
    "    stop=[\"<|end_of_text|>\", \"<|eot_id|>\"],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58ab04f-08ae-4715-a9e1-50992bcea214",
   "metadata": {},
   "source": [
    "### Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49757b24-1aa9-4d55-8fd6-d4a58ae19b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je adore le programmation.\n",
      "\n",
      "(Note: \"programming\" is translated as both an activity (\"je programme\") and also referring specifically to computer science, which I've tried to convey with my translation.)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2005.08 ms\n",
      "llama_print_timings:      sample time =      11.32 ms /    40 runs   (    0.28 ms per token,  3532.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1829.14 ms /    33 tokens (   55.43 ms per token,    18.04 tokens per second)\n",
      "llama_print_timings:        eval time =    4734.94 ms /    39 runs   (  121.41 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =    6614.99 ms /    72 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Je adore le programmation.\\n\\n(Note: \"programming\" is translated as both an activity (\"je programme\") and also referring specifically to computer science, which I\\'ve tried to convey with my translation.)', response_metadata={'finish_reason': 'stop'}, id='run-0eb329b2-d351-4cf6-b355-04d24c09d2cb-0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8698eb87-8f0d-4295-84b5-221f372090f4",
   "metadata": {},
   "source": [
    "### Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65945eef-5d04-4f23-8db6-fdf19a8ec8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Programmieren ist toll! (That's \"Programming is great!\" in case you didn't know.) Do you have any favorite languages or projects? Ich bin hier, um dir zu helfen und über das Thema sprechen! () - I'm here to help and talk about the topic with you!)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   15285.72 ms\n",
      "llama_print_timings:      sample time =      17.26 ms /    61 runs   (    0.28 ms per token,  3534.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     629.70 ms /    16 tokens (   39.36 ms per token,    25.41 tokens per second)\n",
      "llama_print_timings:        eval time =    6864.21 ms /    60 runs   (  114.40 ms per token,     8.74 tokens per second)\n",
      "llama_print_timings:       total time =    7570.66 ms /    76 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Das Programmieren ist toll! (That\\'s \"Programming is great!\" in case you didn\\'t know.) Do you have any favorite languages or projects? Ich bin hier, um dir zu helfen und über das Thema sprechen! () - I\\'m here to help and talk about the topic with you!)', response_metadata={'finish_reason': 'stop'}, id='run-80e2fac4-152f-4ec0-ac5c-3c6e63be345f-0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"German\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0578a02a-a861-476f-a2db-ec496045ab9f",
   "metadata": {},
   "source": [
    "### Structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25927804-e291-4383-9691-cad9c880617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    '''An answer to the user question along with justification for the answer.'''\n",
    "    answer: str\n",
    "    justification: str\n",
    "\n",
    "dict_schema = convert_to_openai_tool(AnswerWithJustification)\n",
    "structured_llm = llm.with_structured_output(dict_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "491fc256-f261-4dd1-8b93-07e558a21b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "answer-kv ::= [\"] [a] [n] [s] [w] [e] [r] [\"] space [:] space string \n",
      "space ::= space_7 \n",
      "string ::= [\"] string_8 [\"] space \n",
      "char ::= [^\"\\] | [\\] char_4 \n",
      "char_4 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "justification-kv ::= [\"] [j] [u] [s] [t] [i] [f] [i] [c] [a] [t] [i] [o] [n] [\"] space [:] space string \n",
      "root ::= [{] space answer-kv [,] space justification-kv [}] space \n",
      "space_7 ::= [ ] | \n",
      "string_8 ::= char string_8 | \n",
      "\n",
      "llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2005.08 ms\n",
      "llama_print_timings:      sample time =     262.96 ms /    30 runs   (    8.77 ms per token,   114.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (     nan ms per token,      nan tokens per second)\n",
      "llama_print_timings:        eval time =    3290.45 ms /    30 runs   (  109.68 ms per token,     9.12 tokens per second)\n",
      "llama_print_timings:       total time =    3670.82 ms /    30 tokens\n"
     ]
    }
   ],
   "source": [
    "result = structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19954aa6-8991-4c06-9931-9f64fb0d833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2625fecc-d242-4110-a5c5-6b8b3c3accc9",
   "metadata": {},
   "source": [
    "### Tool calling\n",
    "\n",
    "* However, it cannot automatically trigger a function/tool. \n",
    "* We need to force it by specifying the 'tool choice' parameter. This parameter is typically formatted as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c85a76ea-1c7b-4df7-81ca-40ba150acf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class WeatherInput(BaseModel):\n",
    "    location: str = Field(description=\"The city and state, e.g. San Francisco, CA\")\n",
    "    unit: str = Field(enum=[\"celsius\", \"fahrenheit\"])\n",
    "\n",
    "@tool(\"get_current_weather\", args_schema=WeatherInput)\n",
    "def get_weather(location: str, unit: str):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    return f\"Now the weather in {location} is 22 {unit}\"\n",
    "\n",
    "llm_with_tools = llm.bind_tools(\n",
    "    tools=[get_weather],\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae99df89-f6b4-41b0-afa6-be02c665f342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "char ::= [^\"\\] | [\\] char_1 \n",
      "char_1 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "location-kv ::= [\"] [l] [o] [c] [a] [t] [i] [o] [n] [\"] space [:] space string \n",
      "space ::= space_7 \n",
      "string ::= [\"] string_8 [\"] space \n",
      "root ::= [{] space location-kv [,] space unit-kv [}] space \n",
      "unit-kv ::= [\"] [u] [n] [i] [t] [\"] space [:] space unit \n",
      "space_7 ::= [ ] | \n",
      "string_8 ::= char string_8 | \n",
      "unit ::= [\"] [c] [e] [l] [s] [i] [u] [s] [\"] | [\"] [f] [a] [h] [r] [e] [n] [h] [e] [i] [t] [\"] \n",
      "\n",
      "llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   15285.72 ms\n",
      "llama_print_timings:      sample time =     137.57 ms /    18 runs   (    7.64 ms per token,   130.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     573.48 ms /    10 tokens (   57.35 ms per token,    17.44 tokens per second)\n",
      "llama_print_timings:        eval time =    1963.75 ms /    17 runs   (  115.51 ms per token,     8.66 tokens per second)\n",
      "llama_print_timings:       total time =    2744.26 ms /    27 tokens\n"
     ]
    }
   ],
   "source": [
    "ai_msg = llm_with_tools.invoke(\n",
    "    \"what is the weather in San Francisco, CA\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa682225-afa0-4384-ae22-2bf0af846058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', response_metadata={'finish_reason': 'tool_calls'}, id='run-e504702e-f0fd-4916-9e40-50efe8ca1549-0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
