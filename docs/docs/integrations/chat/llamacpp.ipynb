{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cf960ef-8cf2-4d2c-8dc4-e7a34b76bd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U --quiet llama-cpp-python tavily-python langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1dbe59-3d32-4bcd-bf4d-851f28447126",
   "metadata": {},
   "source": [
    "### Repo\n",
    "\n",
    "Clone and build Llama.cpp, following instructions here:\n",
    "\n",
    "https://github.com/ggerganov/llama.cpp\n",
    "\n",
    "### Model\n",
    "\n",
    "Download a local LLM, ideally one that is capable of tool calling to use all features discussed below:\n",
    "\n",
    "> Hermes 2 Pro is an upgraded version of Nous Hermes 2, consisting of an updated and cleaned version of the OpenHermes 2.5 Dataset, as well as a newly introduced Function Calling and JSON Mode dataset developed in-house. This new version of Hermes maintains its excellent general task and conversation capabilities - but also excels at Function Calling\n",
    " \n",
    "* Weights: `Hermes-2-Pro-Llama-3-8B-Q8_0.gguf`\n",
    "* Link: https://huggingface.co/NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6462b386-689f-4a46-b0dc-c547a5c84105",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model = \"/Users/rlm/Desktop/Code/llama.cpp/models/Hermes-2-Pro-Llama-3-8B-Q8_0.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1efae91-02a4-4527-9485-16957ae6970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatLlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa40464d-3d97-4222-a426-dfae262f6449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm = ChatLlamaCpp(\n",
    "    temperature=0.3,\n",
    "    model_path=local_model,\n",
    "    n_ctx=10000,\n",
    "    n_gpu_layers=4,\n",
    "    n_batch=200,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    max_tokens=512,\n",
    "    n_threads=multiprocessing.cpu_count() - 1,\n",
    "    callback_manager=CallbackManager(\n",
    "        [StreamingStdOutCallbackHandler()]\n",
    "    ),  # Callbacks support token-wise streaming\n",
    "    streaming=True,\n",
    "    repeat_penalty=1.5,\n",
    "    top_p=0.5,\n",
    "    stop=[\"<|end_of_text|>\", \"<|eot_id|>\"],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58ab04f-08ae-4715-a9e1-50992bcea214",
   "metadata": {},
   "source": [
    "### Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49757b24-1aa9-4d55-8fd6-d4a58ae19b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'aime programmer (in France) or J'adore programmeur(programmer in Quebec). Both translations convey an enthusiastic feeling for loving computer programing, but note there is no direct translation of \"programming\" to French as it's a specific field and the word doesn't exist by itself."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   14486.48 ms\n",
      "llama_print_timings:      sample time =      16.53 ms /    61 runs   (    0.27 ms per token,  3690.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14486.29 ms /    35 tokens (  413.89 ms per token,     2.42 tokens per second)\n",
      "llama_print_timings:        eval time =    6595.61 ms /    60 runs   (  109.93 ms per token,     9.10 tokens per second)\n",
      "llama_print_timings:       total time =   21156.22 ms /    95 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='J\\'aime programmer (in France) or J\\'adore programmeur(programmer in Quebec). Both translations convey an enthusiastic feeling for loving computer programing, but note there is no direct translation of \"programming\" to French as it\\'s a specific field and the word doesn\\'t exist by itself.', response_metadata={'finish_reason': 'stop'}, id='run-f8c421fc-f022-4e47-ab1e-3b7fe80be36e-0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8698eb87-8f0d-4295-84b5-221f372090f4",
   "metadata": {},
   "source": [
    "### Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65945eef-5d04-4f23-8db6-fdf19a8ec8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ich liebe Programmieren.\n",
      "\n",
      "Translation: I (ich) + Love/like (liebe)+ Verb -ing form (-nd, -en or –t as suffix of verb infinitive in german language )+ noun “programming”(Programmierung). So the translation is \"I love programming\" which translates to  Ich Liebe programmierun."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   14486.48 ms\n",
      "llama_print_timings:      sample time =      19.22 ms /    71 runs   (    0.27 ms per token,  3694.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     721.99 ms /    16 tokens (   45.12 ms per token,    22.16 tokens per second)\n",
      "llama_print_timings:        eval time =    8322.44 ms /    70 runs   (  118.89 ms per token,     8.41 tokens per second)\n",
      "llama_print_timings:       total time =    9128.37 ms /    86 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ich liebe Programmieren.\\n\\nTranslation: I (ich) + Love/like (liebe)+ Verb -ing form (-nd, -en or –t as suffix of verb infinitive in german language )+ noun “programming”(Programmierung). So the translation is \"I love programming\" which translates to  Ich Liebe programmierun.', response_metadata={'finish_reason': 'stop'}, id='run-e6cc700a-ac9d-42c5-8ad1-47fe0b4ef4ea-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"German\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0578a02a-a861-476f-a2db-ec496045ab9f",
   "metadata": {},
   "source": [
    "### Structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25927804-e291-4383-9691-cad9c880617d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "char ::= [^\"\\] | [\\] char_1 \n",
      "char_1 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "punchline-kv ::= [\"] [p] [u] [n] [c] [h] [l] [i] [n] [e] [\"] space [:] space string \n",
      "space ::= space_7 \n",
      "string ::= [\"] string_8 [\"] space \n",
      "root ::= [{] space setup-kv [,] space punchline-kv [}] space \n",
      "setup-kv ::= [\"] [s] [e] [t] [u] [p] [\"] space [:] space string \n",
      "space_7 ::= [ ] | \n",
      "string_8 ::= char string_8 | \n",
      "\n",
      "llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14486.48 ms\n",
      "llama_print_timings:      sample time =     296.98 ms /    32 runs   (    9.28 ms per token,   107.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     348.76 ms /     6 tokens (   58.13 ms per token,    17.20 tokens per second)\n",
      "llama_print_timings:        eval time =    3374.11 ms /    31 runs   (  108.84 ms per token,     9.19 tokens per second)\n",
      "llama_print_timings:       total time =    4135.82 ms /    37 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't seagulls fly over the sea?\",\n",
       " 'punchline': \"Because they'd get waterlogged!\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    '''A setup to a joke and the punchline.'''\n",
    "    setup: str\n",
    "    punchline: str\n",
    "\n",
    "dict_schema = convert_to_openai_tool(Joke)\n",
    "structured_llm = llm.with_structured_output(dict_schema)\n",
    "result = structured_llm.invoke(\"Tell me a joke about birds\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2625fecc-d242-4110-a5c5-6b8b3c3accc9",
   "metadata": {},
   "source": [
    "### Tool calling\n",
    "\n",
    "* However, it cannot automatically trigger a function/tool. \n",
    "* We need to force it by specifying the 'tool choice' parameter. This parameter is typically formatted as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d5ea49c-c231-49e7-8414-fce9cd0bf1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "char ::= [^\"\\] | [\\] char_1 \n",
      "char_1 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "location-kv ::= [\"] [l] [o] [c] [a] [t] [i] [o] [n] [\"] space [:] space string \n",
      "space ::= space_7 \n",
      "string ::= [\"] string_8 [\"] space \n",
      "root ::= [{] space location-kv [,] space unit-kv [}] space \n",
      "unit-kv ::= [\"] [u] [n] [i] [t] [\"] space [:] space unit \n",
      "space_7 ::= [ ] | \n",
      "string_8 ::= char string_8 | \n",
      "unit ::= [\"] [c] [e] [l] [s] [i] [u] [s] [\"] | [\"] [f] [a] [h] [r] [e] [n] [h] [e] [i] [t] [\"] \n",
      "\n",
      "llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14486.48 ms\n",
      "llama_print_timings:      sample time =     148.92 ms /    19 runs   (    7.84 ms per token,   127.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     824.90 ms /    17 tokens (   48.52 ms per token,    20.61 tokens per second)\n",
      "llama_print_timings:        eval time =    2194.72 ms /    18 runs   (  121.93 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =    3239.03 ms /    35 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{ \"location\": \"Ho Chi Minh City\", \"unit\":\"celsius\"} '}, 'tool_calls': [{'id': 'call__0_get_current_weather_cmpl-63f51030-8ecd-43e6-b3fa-5e3e2e29ff35', 'type': 'function', 'function': {'name': 'get_current_weather', 'arguments': '{ \"location\": \"Ho Chi Minh City\", \"unit\":\"celsius\"} '}}]}, response_metadata={'token_usage': {'prompt_tokens': 22, 'completion_tokens': 18, 'total_tokens': 40}, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-1abf34ef-8f51-459a-b25b-4204ea18e4a7-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'Ho Chi Minh City', 'unit': 'celsius'}, 'id': 'call__0_get_current_weather_cmpl-63f51030-8ecd-43e6-b3fa-5e3e2e29ff35'}])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class WeatherInput(BaseModel):\n",
    "    location: str = Field(description=\"The city and state, e.g. San Francisco, CA\")\n",
    "    unit: str = Field(enum=[\"celsius\", \"fahrenheit\"])\n",
    "\n",
    "@tool(\"get_current_weather\", args_schema=WeatherInput)\n",
    "def get_weather(location: str, unit: str):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    return f\"Now the weather in {location} is 22 {unit}\"\n",
    "\n",
    "llm_with_tools = llm.bind_tools(\n",
    "    tools=[get_weather],\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\"}},\n",
    ")\n",
    "\n",
    "ai_msg = llm_with_tools.invoke(\n",
    "    \"what is the weather like in HCMC in celsius\",\n",
    ")\n",
    "\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eed1a77c-70be-487c-be5d-a843f502c915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "integer ::= integer_1 space \n",
      "integer_1 ::= integer_2 integral-part \n",
      "integer_2 ::= [-] | \n",
      "integral-part ::= [0-9] | [1-9] integral-part_34 \n",
      "space ::= space_37 \n",
      "integral-part_5 ::= [0-9] integral-part_33 \n",
      "integral-part_6 ::= [0-9] integral-part_32 \n",
      "integral-part_7 ::= [0-9] integral-part_31 \n",
      "integral-part_8 ::= [0-9] integral-part_30 \n",
      "integral-part_9 ::= [0-9] integral-part_29 \n",
      "integral-part_10 ::= [0-9] integral-part_28 \n",
      "integral-part_11 ::= [0-9] integral-part_27 \n",
      "integral-part_12 ::= [0-9] integral-part_26 \n",
      "integral-part_13 ::= [0-9] integral-part_25 \n",
      "integral-part_14 ::= [0-9] integral-part_24 \n",
      "integral-part_15 ::= [0-9] integral-part_23 \n",
      "integral-part_16 ::= [0-9] integral-part_22 \n",
      "integral-part_17 ::= [0-9] integral-part_21 \n",
      "integral-part_18 ::= [0-9] integral-part_20 \n",
      "integral-part_19 ::= [0-9] \n",
      "integral-part_20 ::= integral-part_19 | \n",
      "integral-part_21 ::= integral-part_18 | \n",
      "integral-part_22 ::= integral-part_17 | \n",
      "integral-part_23 ::= integral-part_16 | \n",
      "integral-part_24 ::= integral-part_15 | \n",
      "integral-part_25 ::= integral-part_14 | \n",
      "integral-part_26 ::= integral-part_13 | \n",
      "integral-part_27 ::= integral-part_12 | \n",
      "integral-part_28 ::= integral-part_11 | \n",
      "integral-part_29 ::= integral-part_10 | \n",
      "integral-part_30 ::= integral-part_9 | \n",
      "integral-part_31 ::= integral-part_8 | \n",
      "integral-part_32 ::= integral-part_7 | \n",
      "integral-part_33 ::= integral-part_6 | \n",
      "integral-part_34 ::= integral-part_5 | \n",
      "magic-function-input-kv ::= [\"] [m] [a] [g] [i] [c] [_] [f] [u] [n] [c] [t] [i] [o] [n] [_] [i] [n] [p] [u] [t] [\"] space [:] space integer \n",
      "root ::= [{] space magic-function-input-kv [}] space \n",
      "space_37 ::= [ ] | \n",
      "\n",
      "llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14486.48 ms\n",
      "llama_print_timings:      sample time =      78.16 ms /    10 runs   (    7.82 ms per token,   127.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1331.98 ms /    13 tokens (  102.46 ms per token,     9.76 tokens per second)\n",
      "llama_print_timings:        eval time =    1560.89 ms /     9 runs   (  173.43 ms per token,     5.77 tokens per second)\n",
      "llama_print_timings:       total time =    3009.29 ms /    22 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_magic_function', 'arguments': '{\"magic_function_input\": 3}'}, 'tool_calls': [{'id': 'call__0_get_magic_function_cmpl-d7088e2e-02c4-4505-a051-2cc37878d6a8', 'type': 'function', 'function': {'name': 'get_magic_function', 'arguments': '{\"magic_function_input\": 3}'}}]}, response_metadata={'token_usage': {'prompt_tokens': 18, 'completion_tokens': 9, 'total_tokens': 27}, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-89867ecb-1d3b-4242-8f37-3e896c1f1f81-0', tool_calls=[{'name': 'get_magic_function', 'args': {'magic_function_input': 3}, 'id': 'call__0_get_magic_function_cmpl-d7088e2e-02c4-4505-a051-2cc37878d6a8'}])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MagicFunctionInput(BaseModel):\n",
    "    magic_function_input: int = Field(description=\"The input value for magic function\")\n",
    "\n",
    "@tool(\"get_magic_function\", args_schema=MagicFunctionInput)\n",
    "def magic_function(magic_function_input: int):\n",
    "    \"\"\"Get the value of magic function for an input.\"\"\"\n",
    "    return value + 2\n",
    "\n",
    "llm_with_tools = llm.bind_tools(\n",
    "    tools=[magic_function],\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_magic_function\"}},\n",
    ")\n",
    "\n",
    "ai_msg = llm_with_tools.invoke(\n",
    "    \"What is magic function of 3?\",\n",
    ")\n",
    "\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ffba8b-f2cd-46f7-bc8f-1c5f07d488aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "TAVILY_API_KEY = getpass()\n",
    "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c6dd37db-86d3-47e8-b2f7-05ef3d264bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "char ::= [^\"\\] | [\\] char_1 \n",
      "char_1 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "root ::= [{] space search-query-kv [}] space \n",
      "space ::= space_6 \n",
      "search-query-kv ::= [\"] [s] [e] [a] [r] [c] [h] [_] [q] [u] [e] [r] [y] [\"] space [:] space string \n",
      "string ::= [\"] string_7 [\"] space \n",
      "space_6 ::= [ ] | \n",
      "string_7 ::= char string_7 | \n",
      "\n",
      "llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14486.48 ms\n",
      "llama_print_timings:      sample time =     169.86 ms /    18 runs   (    9.44 ms per token,   105.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     800.94 ms /    19 tokens (   42.15 ms per token,    23.72 tokens per second)\n",
      "llama_print_timings:        eval time =    1740.33 ms /    17 runs   (  102.37 ms per token,     9.77 tokens per second)\n",
      "llama_print_timings:       total time =    2777.51 ms /    36 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_web_search', 'arguments': '{ \"search_query\": \"weather Ho Chi Minh City, Vietnam today Celsius\" }'}, 'tool_calls': [{'id': 'call__0_get_web_search_cmpl-8c9d8e91-4f6b-4c6d-b994-24297e21c2c6', 'type': 'function', 'function': {'name': 'get_web_search', 'arguments': '{ \"search_query\": \"weather Ho Chi Minh City, Vietnam today Celsius\" }'}}]}, response_metadata={'token_usage': {'prompt_tokens': 22, 'completion_tokens': 17, 'total_tokens': 39}, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-455c7255-838b-4042-ac59-227cc04b0606-0', tool_calls=[{'name': 'get_web_search', 'args': {'search_query': 'weather Ho Chi Minh City, Vietnam today Celsius'}, 'id': 'call__0_get_web_search_cmpl-8c9d8e91-4f6b-4c6d-b994-24297e21c2c6'}])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WebSearchInput(BaseModel):\n",
    "    search_query: str = Field(description=\"Search query to pass to web search\")\n",
    "    \n",
    "@tool(\"get_web_search\", args_schema=WebSearchInput)\n",
    "def get_web_search(search_query: str): #\n",
    "    \"\"\"Get web search results for a query\"\"\"\n",
    "    web_search_tool = TavilySearchResults()\n",
    "    docs = web_search_tool.invoke({\"query\": search_query})\n",
    "    return docs\n",
    "\n",
    "llm_with_tools = llm.bind_tools(\n",
    "    tools=[get_web_search],\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_web_search\"}},\n",
    ")\n",
    "\n",
    "ai_msg = llm_with_tools.invoke(\n",
    "    \"what is the weather like in HCMC in celsius\",\n",
    ")\n",
    "\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "04e94e0c-2e23-404b-befb-f8e57865b834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "char ::= [^\"\\] | [\\] char_1 \n",
      "char_1 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "root ::= [{] space search-query-kv [}] space \n",
      "space ::= space_6 \n",
      "search-query-kv ::= [\"] [s] [e] [a] [r] [c] [h] [_] [q] [u] [e] [r] [y] [\"] space [:] space string \n",
      "string ::= [\"] string_7 [\"] space \n",
      "space_6 ::= [ ] | \n",
      "string_7 ::= char string_7 | \n",
      "\n",
      "llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14486.48 ms\n",
      "llama_print_timings:      sample time =     124.74 ms /    14 runs   (    8.91 ms per token,   112.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     798.52 ms /    16 tokens (   49.91 ms per token,    20.04 tokens per second)\n",
      "llama_print_timings:        eval time =    1307.02 ms /    13 runs   (  100.54 ms per token,     9.95 tokens per second)\n",
      "llama_print_timings:       total time =    2279.94 ms /    29 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_web_search', 'arguments': '{ \"search_query\": \"first nfl draft pick\" } '}, 'tool_calls': [{'id': 'call__0_get_web_search_cmpl-710322d0-3264-4fcc-a2a3-9a0e9ea2c4ed', 'type': 'function', 'function': {'name': 'get_web_search', 'arguments': '{ \"search_query\": \"first nfl draft pick\" } '}}]}, response_metadata={'token_usage': {'prompt_tokens': 21, 'completion_tokens': 13, 'total_tokens': 34}, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f2e759b1-eba1-47a5-8e40-96fd2997391b-0', tool_calls=[{'name': 'get_web_search', 'args': {'search_query': 'first nfl draft pick'}, 'id': 'call__0_get_web_search_cmpl-710322d0-3264-4fcc-a2a3-9a0e9ea2c4ed'}])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg = llm_with_tools.invoke(\n",
    "    \"who was the first draft pick in the NFL draft?\",\n",
    ")\n",
    "\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b6b6bcfa-84e6-4a5a-a70e-744c5aaec31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To provide you with accurate and up-to-date information about Ho Chi Minh City's current temperature, I would need to access an online source or API that provides real-time data on global temperatures.\n",
      "\n",
      "One such service could be OpenWeatherMap (https://openweathermap.org/), which offers a range of APIs for accessing weather-related datasets. \n",
      "\n",
      "To get the latest 5-day forecast in Celsius degrees and other details like humidity, wind speed etc., you can use their One Call API by providing your desired location coordinates or city name.\n",
      "\n",
      "Here's an example Python code snippet using OpenWeatherMap’s one call api:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "\n",
      "# Replace YOUR_API_KEY with a valid access key from https://openweathermap.org/appid \n",
      "API_key = 'YOUR_APP_ID'\n",
      "\n",
      "def get_weather_data(city_name):\n",
      "    base_url ='http://api.openweathermap.org/data/2.5/weather'\n",
      "    \n",
      "   # Get the current weather data for given city\n",
      "    response= requests.get(base_url, params={'q':city_name,'appid' : API_KEY})\n",
      "  \n",
      "  if(response.status_code ==200): \n",
      "      return (response.json())\n",
      "      \n",
      "# Call function to get latest temperature in Celsius and other details like humidity etc. of HCMC.\n",
      "weather_data =get_weather_data('Ho Chi Minh City')\n",
      "\n",
      "print(weather_data)\n",
      "```\n",
      "\n",
      "Please replace YOUR_APP_ID with your valid access key from OpenWeatherMap.\n",
      "\n",
      "This code will fetch the current weather data for Ho chi minh city, including its latest temperature in Celsius and other details like humidity etc."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   14486.48 ms\n",
      "llama_print_timings:      sample time =      87.81 ms /   310 runs   (    0.28 ms per token,  3530.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2011.78 ms /    11 tokens (  182.89 ms per token,     5.47 tokens per second)\n",
      "llama_print_timings:        eval time =   39709.35 ms /   309 runs   (  128.51 ms per token,     7.78 tokens per second)\n",
      "llama_print_timings:       total time =   42158.45 ms /   320 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"To provide you with accurate and up-to-date information about Ho Chi Minh City's current temperature, I would need to access an online source or API that provides real-time data on global temperatures.\\n\\nOne such service could be OpenWeatherMap (https://openweathermap.org/), which offers a range of APIs for accessing weather-related datasets. \\n\\nTo get the latest 5-day forecast in Celsius degrees and other details like humidity, wind speed etc., you can use their One Call API by providing your desired location coordinates or city name.\\n\\nHere's an example Python code snippet using OpenWeatherMap’s one call api:\\n\\n```python\\nimport requests\\n\\n# Replace YOUR_API_KEY with a valid access key from https://openweathermap.org/appid \\nAPI_key = 'YOUR_APP_ID'\\n\\ndef get_weather_data(city_name):\\n    base_url ='http://api.openweathermap.org/data/2.5/weather'\\n    \\n   # Get the current weather data for given city\\n    response= requests.get(base_url, params={'q':city_name,'appid' : API_KEY})\\n  \\n  if(response.status_code ==200): \\n      return (response.json())\\n      \\n# Call function to get latest temperature in Celsius and other details like humidity etc. of HCMC.\\nweather_data =get_weather_data('Ho Chi Minh City')\\n\\nprint(weather_data)\\n```\\n\\nPlease replace YOUR_APP_ID with your valid access key from OpenWeatherMap.\\n\\nThis code will fetch the current weather data for Ho chi minh city, including its latest temperature in Celsius and other details like humidity etc.\", response_metadata={'finish_reason': 'stop'}, id='run-265af26d-2a00-4c0c-ab94-cfc927bbcdc0-0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test w/o tool choice specified\n",
    "llm_with_tools = llm.bind_tools(\n",
    "    tools=[get_web_search],\n",
    ")\n",
    "\n",
    "ai_msg = llm_with_tools.invoke(\n",
    "    \"what is the weather like in HCMC in celsius. use a tool call.\",\n",
    ")\n",
    "\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1122ee-6545-4d0d-a70a-48f179b605ae",
   "metadata": {},
   "source": [
    "#### Tests\n",
    "\n",
    "Test conversion of working weather example to web search. \n",
    "\n",
    "* `Class name change:`  `WeatherInput` -> `WebSearchInput`, no effect.\n",
    "* `Tool name change:`  `get_current_weather` -> `get_web_search`, no effect.\n",
    "* `Function name change`: `get_weather` -> `get_web_search`,  \"location\": \"Ho Chi Minh City\" -> Some unreliability (returns lat, lng)\n",
    "* `Class parameter removal`: `unit: str = Field(enum=[\"celsius\", \"fahrenheit\"])` -> `None`, no effect.\n",
    "* `Function description`: ` \"\"\"Get the current weather in a given location\"\"\"` -> `\"\"\"Search about a given location\"\"\"`, no effect\n",
    "* `Class parameter name`:  `location` -> `value`, **BREAKS.** Attempts to answer the question / spams the input payload.\n",
    "* `Class parameter name`:  `location` -> `search_query`, **OK** Formulates a good query as input payload.\n",
    "\n",
    "Therefore, apparently very sensitive to the selection of the input field name in the class schema :) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f132d45-ed7a-4d6d-9686-a16afdaf5b0c",
   "metadata": {},
   "source": [
    "### Agents\n",
    "\n",
    "Try LangGraph [tool calling agent](https://github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/langgraph-tool-calling-agent.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "afbaaa4c-2ec3-4df7-bf8d-d594aecba085",
   "metadata": {},
   "outputs": [],
   "source": [
    "### State\n",
    "\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "24ada449-af1b-47be-bd04-89f70dad95cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assistant\n",
    "\n",
    "from datetime import datetime\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "\n",
    "class Assistant:\n",
    "    \n",
    "    def __init__(self, runnable: Runnable):\n",
    "        self.runnable = runnable\n",
    "\n",
    "    def __call__(self, state: State, config: RunnableConfig):\n",
    "        while True:\n",
    "            # Get any user-provided configs \n",
    "            image_url = config['configurable'].get(\"image_url\", None)\n",
    "            # Append to state\n",
    "            state = {**state, \"image_url\": image_url}\n",
    "            # Invoke the tool-calling LLM\n",
    "            result = self.runnable.invoke(state)\n",
    "            # If it is a tool call -> response is valid\n",
    "            # If it has meaninful text -> response is valid\n",
    "            # Otherwise, we re-prompt it b/c response is not meaninful\n",
    "            if not result.tool_calls and (\n",
    "                not result.content\n",
    "                or isinstance(result.content, list)\n",
    "                and not result.content[0].get(\"text\")\n",
    "            ):\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                break\n",
    "        return {\"messages\": result}\n",
    "\n",
    "# Prompt \n",
    "primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant for with two tools: (1) web search, \"\n",
    "            \"(2) a custom, magic_function. Use one of these tools based upon \" \n",
    "            \" the user input. \"\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ").partial(time=datetime.now())\n",
    "\n",
    "# LIMITATION: ** CANNOT INVOKE MULTIPLE TOOLS **\n",
    "# Not reliable w/o tool_choice\n",
    "llm_with_tools = llm.bind_tools(\n",
    "    tools=[magic_function, get_web_search],\n",
    ")\n",
    "\n",
    "# Tools \n",
    "tools = [get_web_search]\n",
    "\n",
    "# Test agent with single tool\n",
    "llm_with_tools = llm.bind_tools(\n",
    "    tools=tools,\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_web_search\"}},\n",
    ")\n",
    "\n",
    "assistant_runnable = primary_assistant_prompt | llm_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6427dd13-1230-4714-b7b4-a4f2b239c6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph utils\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def create_tool_node_with_fallback(tools: list) -> dict:\n",
    "    return ToolNode(tools).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
    "    )\n",
    "\n",
    "def _print_event(event: dict, _printed: set, max_length=1500):\n",
    "    current_state = event.get(\"dialog_state\")\n",
    "    if current_state:\n",
    "        print(f\"Currently in: \", current_state[-1])\n",
    "    message = event.get(\"messages\")\n",
    "    if message:\n",
    "        if isinstance(message, list):\n",
    "            message = message[-1]\n",
    "        if message.id not in _printed:\n",
    "            msg_repr = message.pretty_repr(html=True)\n",
    "            if len(msg_repr) > max_length:\n",
    "                msg_repr = msg_repr[:max_length] + \" ... (truncated)\"\n",
    "            print(msg_repr)\n",
    "            _printed.add(message.id)\n",
    "\n",
    "def handle_tool_error(state) -> dict:\n",
    "    error = state.get(\"error\")\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n",
    "                tool_call_id=tc[\"id\"],\n",
    "            )\n",
    "            for tc in tool_calls\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "29049f09-85cb-4ac0-acad-abb4a1900658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Graph\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", Assistant(assistant_runnable))\n",
    "builder.add_node(\"tools\", create_tool_node_with_fallback(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.set_entry_point(\"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition, \n",
    "    # \"tools\" calls one of our tools. END causes the graph to terminate (and respond to the user)\n",
    "    {\"tools\": \"tools\", END: END},\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "# The checkpointer lets the graph persist its state\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "graph = builder.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "80ea981e-4c59-431d-9e78-b3cbdfeb1e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"What is magic_function(3)\",\n",
    "             \"What is the weather in SF?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "87ee7c17-e859-4416-b2dd-5f10b0aa6806",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "char ::= [^\"\\] | [\\] char_1 \n",
      "char_1 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "root ::= [{] space search-query-kv [}] space \n",
      "space ::= space_6 \n",
      "search-query-kv ::= [\"] [s] [e] [a] [r] [c] [h] [_] [q] [u] [e] [r] [y] [\"] space [:] space string \n",
      "string ::= [\"] string_7 [\"] space \n",
      "space_6 ::= [ ] | \n",
      "string_7 ::= char string_7 | \n",
      "\n",
      "llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the weather in SF?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14486.48 ms\n",
      "llama_print_timings:      sample time =     109.56 ms /    12 runs   (    9.13 ms per token,   109.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1999.90 ms /    57 tokens (   35.09 ms per token,    28.50 tokens per second)\n",
      "llama_print_timings:        eval time =    1592.15 ms /    11 runs   (  144.74 ms per token,     6.91 tokens per second)\n",
      "llama_print_timings:       total time =    3749.65 ms /    68 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_web_search (call__0_get_web_search_cmpl-cc27cc8d-0e39-482a-a991-33c8a33a5522)\n",
      " Call ID: call__0_get_web_search_cmpl-cc27cc8d-0e39-482a-a991-33c8a33a5522\n",
      "  Args:\n",
      "    search_query: weather San Francisco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in LangChainTracer.on_tool_end callback: TracerException(\"Found chain run at ID d810e414-dd2a-4243-975f-c910244b7e30, but expected {'tool'} run.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_web_search\n",
      "\n",
      "[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.78, 'lon': -122.42, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1718254106, 'localtime': '2024-06-12 21:48'}, 'current': {'last_updated_epoch': 1718253900, 'last_updated': '2024-06-12 21:45', 'temp_c': 15.6, 'temp_f': 60.1, 'is_day': 0, 'condition': {'text': 'Partly cloudy', 'icon': '//cdn.weatherapi.com/weather/64x64/night/116.png', 'code': 1003}, 'wind_mph': 6.9, 'wind_kph': 11.2, 'wind_degree': 310, 'wind_dir': 'NW', 'pressure_mb': 1012.0, 'pressure_in': 29.89, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 80, 'cloud': 25, 'feelslike_c': 15.6, 'feelslike_f': 60.1, 'windchill_c': 10.5, 'windchill_f': 50.9, 'heatindex_c': 12.6, 'heatindex_f': 54.6, 'dewpoint_c': 9.3, 'dewpoint_f': 48.8, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 1.0, 'gust_mph': 19.9, 'gust_kph': 32.0}}\"}, {\"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2024/\", \"content\": \"Extended weather forecast in San Francisco. Hourly Week 10 days 14 days 30 days Year. Detailed \\u26a1 San Francisco Weather Forecast for June 2024 - day/night \\ud83c\\udf21\\ufe0f temperatures, precipitations - World-Weather.info.\"}, {\"url\": \"https://www.weathertab.com/en/c/e/06/united-states/california/san-francisco/\", \"content\": \"Explore compre ... (truncated)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"NoneType\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 14\u001b[0m\n\u001b[1;32m      5\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: thread_id,\n\u001b[1;32m      8\u001b[0m     }\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     11\u001b[0m events \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m     12\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, questions[\u001b[38;5;241m1\u001b[39m])}, config, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevents\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_print_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_printed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/llama2/lib/python3.11/site-packages/langgraph/pregel/__init__.py:963\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m fut, task\n\u001b[1;32m    962\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 963\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
      "File \u001b[0;32m~/miniforge3/envs/llama2/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1489\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m   1487\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1488\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1489\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1492\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m   1494\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/llama2/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/miniforge3/envs/llama2/lib/python3.11/site-packages/langgraph/pregel/retry.py:66\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     64\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Code/langchain-main/langchain/libs/core/langchain_core/runnables/base.py:2493\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2489\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m   2490\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2491\u001b[0m )\n\u001b[1;32m   2492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2493\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2494\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2495\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/miniforge3/envs/llama2/lib/python3.11/site-packages/langgraph/utils.py:95\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[1;32m     94\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m---> 95\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[90], line 19\u001b[0m, in \u001b[0;36mAssistant.__call__\u001b[0;34m(self, state, config)\u001b[0m\n\u001b[1;32m     17\u001b[0m state \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstate, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_url\u001b[39m\u001b[38;5;124m\"\u001b[39m: image_url}\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Invoke the tool-calling LLM\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunnable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# If it is a tool call -> response is valid\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# If it has meaninful text -> response is valid\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Otherwise, we re-prompt it b/c response is not meaninful\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mtool_calls \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m ):\n",
      "File \u001b[0;32m~/Desktop/Code/langchain-main/langchain/libs/core/langchain_core/runnables/base.py:2495\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2494\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2495\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2496\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/Code/langchain-main/langchain/libs/core/langchain_core/runnables/base.py:4558\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4553\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4554\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4555\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4556\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4557\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4558\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4559\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4560\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4561\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4562\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/langchain-main/langchain/libs/core/langchain_core/language_models/chat_models.py:170\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    166\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    167\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    169\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 170\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    180\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Desktop/Code/langchain-main/langchain/libs/core/langchain_core/language_models/chat_models.py:599\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    593\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    598\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/langchain-main/langchain/libs/core/langchain_core/language_models/chat_models.py:456\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    455\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    457\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    458\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    460\u001b[0m ]\n\u001b[1;32m    461\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/Desktop/Code/langchain-main/langchain/libs/core/langchain_core/language_models/chat_models.py:446\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 446\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m         )\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Desktop/Code/langchain-main/langchain/libs/core/langchain_core/language_models/chat_models.py:671\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 671\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Code/langchain-main/langchain/libs/community/langchain_community/chat_models/llamacpp.py:497\u001b[0m, in \u001b[0;36mChatLlamaCpp._generate\u001b[0;34m(self, messages, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generate_from_stream(stream_iter)\n\u001b[1;32m    495\u001b[0m message_dicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages)\n\u001b[0;32m--> 497\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_chat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/miniforge3/envs/llama2/lib/python3.11/site-packages/llama_cpp/llama.py:1734\u001b[0m, in \u001b[0;36mLlama.create_chat_completion\u001b[0;34m(self, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs)\u001b[0m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate a chat completion from a list of messages.\u001b[39;00m\n\u001b[1;32m   1699\u001b[0m \n\u001b[1;32m   1700\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;124;03m    Generated chat completion or a stream of chat completion chunks.\u001b[39;00m\n\u001b[1;32m   1730\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1731\u001b[0m handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_handler \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_handlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format) \u001b[38;5;129;01mor\u001b[39;00m llama_chat_format\u001b[38;5;241m.\u001b[39mget_chat_completion_handler(\n\u001b[1;32m   1732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format\n\u001b[1;32m   1733\u001b[0m )\n\u001b[0;32m-> 1734\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllama\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/llama2/lib/python3.11/site-packages/llama_cpp/llama_chat_format.py:544\u001b[0m, in \u001b[0;36mchat_formatter_to_chat_completion_handler.<locals>.chat_completion_handler\u001b[0;34m(llama, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat_completion_handler\u001b[39m(\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    508\u001b[0m     llama: llama\u001b[38;5;241m.\u001b[39mLlama,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m     Iterator[llama_types\u001b[38;5;241m.\u001b[39mCreateChatCompletionStreamResponse],\n\u001b[1;32m    543\u001b[0m ]:\n\u001b[0;32m--> 544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mchat_formatter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mprompt\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/llama2/lib/python3.11/site-packages/llama_cpp/llama_chat_format.py:214\u001b[0m, in \u001b[0;36mJinja2ChatFormatter.__call__\u001b[0;34m(self, messages, functions, function_call, tools, tool_choice, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_exception\u001b[39m(message: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n\u001b[0;32m--> 214\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_environment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m stopping_criteria \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_token_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/llama2/lib/python3.11/site-packages/jinja2/environment.py:1301\u001b[0m, in \u001b[0;36mTemplate.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_render_func(ctx))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1301\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/llama2/lib/python3.11/site-packages/jinja2/environment.py:936\u001b[0m, in \u001b[0;36mEnvironment.handle_exception\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;124;03mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[0;32m--> 936\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m rewrite_traceback_stack(source\u001b[38;5;241m=\u001b[39msource)\n",
      "File \u001b[0;32m<template>:2\u001b[0m, in \u001b[0;36mtop-level template code\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"NoneType\") to str"
     ]
    }
   ],
   "source": [
    "import uuid \n",
    "_printed = set()\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "events = graph.stream(\n",
    "    {\"messages\": (\"user\", questions[1])}, config, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    _print_event(event, _printed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00432004-14d4-4fc0-9dd5-9e80231eb586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
