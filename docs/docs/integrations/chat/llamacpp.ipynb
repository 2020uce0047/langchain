{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b1dbe59-3d32-4bcd-bf4d-851f28447126",
   "metadata": {},
   "source": [
    "### Repo\n",
    "\n",
    "Clone and build Llama.cpp, following instructions here:\n",
    "\n",
    "https://github.com/ggerganov/llama.cpp\n",
    "\n",
    "### Model\n",
    "\n",
    "Download a local LLM, ideally one that is capable of tool calling to use all features discussed below:\n",
    " \n",
    "* Weights: `meta-llama-3-8b-instruct.Q8_0.gguf`\n",
    "* Link: https://huggingface.co/SanctumAI/Meta-Llama-3-8B-Instruct-GGUF\n",
    "\n",
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16d75820-1979-48f4-9310-9262db05952a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'libs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommunity\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllamacpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatLlamaCpp\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'libs'"
     ]
    }
   ],
   "source": [
    "from libs.community.langchain_community.chat_models.llamacpp import ChatLlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66c7e1a-fb30-4636-a99e-c249e05bfbde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9191f3a0-0525-4c03-ba25-12ff34c0fd36",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ChatLlamaCpp' from 'langchain_community.chat_models' (/Users/rlm/Desktop/Code/langchain-main/langchain/libs/community/langchain_community/chat_models/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CallbackManager\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming_stdout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StreamingStdOutCallbackHandler\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatLlamaCpp\n\u001b[1;32m      7\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatLlamaCpp(\n\u001b[1;32m      8\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m,\n\u001b[1;32m      9\u001b[0m     model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/rlm/Desktop/Code/llama.cpp/models/meta-llama-3-8b-instruct.Q8_0.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     23\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ChatLlamaCpp' from 'langchain_community.chat_models' (/Users/rlm/Desktop/Code/langchain-main/langchain/libs/community/langchain_community/chat_models/__init__.py)"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.chat_models import ChatLlamaCpp\n",
    "\n",
    "llm = ChatLlamaCpp(\n",
    "    temperature=0.3,\n",
    "    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/models/meta-llama-3-8b-instruct.Q8_0.gguf\",\n",
    "    n_ctx=10000,\n",
    "    n_gpu_layers=4,\n",
    "    n_batch=200,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    max_tokens=512,\n",
    "    n_threads=multiprocessing.cpu_count() - 1,\n",
    "    callback_manager=CallbackManager(\n",
    "        [StreamingStdOutCallbackHandler()]\n",
    "    ),  # Callbacks support token-wise streaming\n",
    "    streaming=True,\n",
    "    repeat_penalty=1.5,\n",
    "    top_p=0.5,\n",
    "    stop=[\"<|end_of_text|>\", \"<|eot_id|>\"],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58ab04f-08ae-4715-a9e1-50992bcea214",
   "metadata": {},
   "source": [
    "### Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49757b24-1aa9-4d55-8fd6-d4a58ae19b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msga\\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8698eb87-8f0d-4295-84b5-221f372090f4",
   "metadata": {},
   "source": [
    "### Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65945eef-5d04-4f23-8db6-fdf19a8ec8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"German\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2625fecc-d242-4110-a5c5-6b8b3c3accc9",
   "metadata": {},
   "source": [
    "### Tool calling\n",
    "\n",
    "* However, it cannot automatically trigger a function/tool. \n",
    "* We need to force it by specifying the 'tool choice' parameter. This parameter is typically formatted as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a76ea-1c7b-4df7-81ca-40ba150acf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class WeatherInput(BaseModel):\n",
    "    location: str = Field(description=\"The city and state, e.g. San Francisco, CA\")\n",
    "    unit: str = Field(enum=[\"celsius\", \"fahrenheit\"])\n",
    "\n",
    "@tool(\"get_current_weather\", args_schema=WeatherInput)\n",
    "def get_weather(location: str, unit: str):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    return f\"Now the weather in {location} is 22 {unit}\"\n",
    "\n",
    "\n",
    "llm_with_tools = llm.bind_tools(\n",
    "    tools=[get_weather],\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\"}},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
